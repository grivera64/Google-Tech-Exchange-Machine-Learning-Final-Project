{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWM79lj2LYI6"
      },
      "source": [
        "# Giovanni & Oscar | ML Final Project | Milestone 3\n",
        "\n",
        "Continuation of Milestone 2's Ipynb\n",
        "\n",
        "## Points (10 pts)\n",
        "## Tasks:\n",
        "\n",
        "1. Add a section to your colab with at least 2 different methods for learning about the data and understanding the differences between the classes.\n",
        "    > This can include visualization like histograms or tables/lists of features and their counts in each class, for example.\n",
        "    > The goal of data analysis is to help you understand the task more deeply.\n",
        "\n",
        "2. Everyone should submit a colab via Classroom even though we expect partners to submit identical copies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPFnCCd7O6fN"
      },
      "source": [
        "### 0. Setup Dependencies and Milestone 2 Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiwopORsO-yC",
        "outputId": "9fa64ace-c88a-4c32-c64c-bde904a5b271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/2.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.1 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Ensure that we have the newest version of pip installed\n",
        "%pip install -q --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn1OWO_1PRrO",
        "outputId": "e675bcef-d154-4dca-c5a0-d2ae7d03f3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "%pip install -q numpy\n",
        "%pip install -q pandas\n",
        "%pip install -q matplotlib\n",
        "%pip install -q seaborn\n",
        "\n",
        "# Helps avoid showing plots in a separate line\n",
        "# %matplotlib inline\n",
        "\n",
        "%pip install -q scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ycGSQ31nQRa8"
      },
      "outputs": [],
      "source": [
        "# Import the modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "\n",
        "# Set the styling of the plt plots to darkgrid\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Removes error messsages and sets precision to 3 decimal places\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJCd5JzUNNuk"
      },
      "source": [
        "#### Load the Data in a Colab Notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TpMPgXbCEiM0",
        "outputId": "6664440b-bebd-46d3-b6ba-53c4359f659d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ct512u3DuP7E",
        "outputId": "4f519dd4-c116-4b7f-9283-a1a12cedb16b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bc09a2a52d01>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the Data Frames from the Training and Testing Data Frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path_to_csv}/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path_to_csv}/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/techexchange-2023-ml-project/train.csv'"
          ]
        }
      ],
      "source": [
        "# Please update the Path here to the location of your train.csv and test.csv files\n",
        "path_to_csv = 'Data/techexchange-2023-ml-project'\n",
        "\n",
        "# Load the Data Frames from the Training and Testing Data Frame\n",
        "train_df = pd.read_csv(f'{path_to_csv}/train.csv')\n",
        "test_df = pd.read_csv(f'{path_to_csv}/test.csv')\n",
        "display(train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GwPDMJdEMBv"
      },
      "outputs": [],
      "source": [
        "# Separate Data Frame for input and outputs\n",
        "\n",
        "input_names = ['id', 'keyword', 'location', 'text']\n",
        "input_df = train_df[input_names]\n",
        "display(input_df.head())\n",
        "\n",
        "output_names = ['target']\n",
        "output_df = train_df[output_names]\n",
        "display(output_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVDoy9CHEMBv"
      },
      "outputs": [],
      "source": [
        "# Convert into numpy data\n",
        "X_data = input_df.to_numpy()\n",
        "Y_data = output_df.to_numpy().flatten()\n",
        "\n",
        "print(X_data[:5])\n",
        "print(Y_data[:5])\n",
        "\n",
        "print(X_data.shape)\n",
        "print(Y_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58XYeXJpEMBw"
      },
      "source": [
        "#### Split the training data into 90% training and 10% for validation (your experiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WbYSErFEMBw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, train_size=0.90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPoIODunEMBw"
      },
      "outputs": [],
      "source": [
        "# Going through a few examples in the training split\n",
        "\n",
        "for index in range(2):\n",
        "    print('Text')\n",
        "    print(X_train[index])\n",
        "    print()\n",
        "    \n",
        "    print('Is Natural Disaster?')\n",
        "    print(Y_train[index], 'Yes' if Y_train[index] == 1 else 'No')\n",
        "    print('======================')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0O6NRVPEMBw"
      },
      "source": [
        "#### Testing and Submitting a Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDF2U1mmEMBx"
      },
      "outputs": [],
      "source": [
        "# Returns a positive result, regardless of the input\n",
        "def baseline_model(text_inputs):\n",
        "    return 1\n",
        "\n",
        "baseline_model_np = np.vectorize(baseline_model, signature='(n) -> ()')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8wkuM5PEMBx"
      },
      "outputs": [],
      "source": [
        "baseline_predictions_train = baseline_model_np(X_train)\n",
        "\n",
        "for i in range(5):\n",
        "    print('Input:')\n",
        "    print(X_train[i])\n",
        "    print()\n",
        "\n",
        "    print('Output')\n",
        "    print(Y_train[i])\n",
        "    print()\n",
        "\n",
        "    print('Prediction')\n",
        "    print(baseline_predictions_train[i])\n",
        "    print('==================')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CTGewujEMBx"
      },
      "outputs": [],
      "source": [
        "# Calculates Log Loss\n",
        "def calculate_loss(labels, predictions):\n",
        "    epsilon = 0.000001  # Prevents taking the natural log of non-positive values\n",
        "    ce_values = -labels * np.log(predictions + epsilon) - (1 - labels) * np.log(1 - predictions + epsilon)\n",
        "    loss = ce_values.mean()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1HFzwY8EMBx"
      },
      "outputs": [],
      "source": [
        "training_loss = calculate_loss(Y_train, baseline_predictions_train)\n",
        "print('Training Loss:', training_loss)\n",
        "\n",
        "# Run on the validation data\n",
        "baseline_predictions_test = baseline_model_np(X_test)\n",
        "testing_loss = calculate_loss(Y_test, baseline_predictions_test)\n",
        "print('Testing Loss:', testing_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dND6V3tVEMBx"
      },
      "outputs": [],
      "source": [
        "# Create the submission CSV file for our Kaggle submission\n",
        "def save_to_submissions_csv(text_inputs, prediction_labels):\n",
        "    print('Generating \"submission.csv\" file...')\n",
        "\n",
        "    # Extract the ids of the text inputs and flatten to a 1D ndarray\n",
        "    test_ids = text_inputs[:,0].flatten()\n",
        "\n",
        "    # Write the submission file and save to 'submission.csv'\n",
        "    np.savetxt(\n",
        "        'submission.csv',\n",
        "        np.rec.fromarrays([test_ids, prediction_labels]),\n",
        "        fmt=['%s', '%d'],\n",
        "        delimiter=',',\n",
        "        header='id,target',\n",
        "        comments=''\n",
        "    )\n",
        "\n",
        "    # Show success!\n",
        "    print('Successfully created \"submission.csv\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9BZnRTHEMBy"
      },
      "outputs": [],
      "source": [
        "# Reformat the single training dataframe to an input dataframe\n",
        "input_names = ['id', 'keyword', 'location', 'text']\n",
        "test_input_df = test_df[input_names]\n",
        "\n",
        "# Reformat the input dataframe into a numpy array for running through our model\n",
        "test_input_np = test_input_df.to_numpy()\n",
        "\n",
        "# Predict by using the baseline model on the test input and save to a .csv\n",
        "baseline_predictions_test = baseline_model_np(test_input_np)\n",
        "save_to_submissions_csv(test_input_np, baseline_predictions_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5bAW6fYEMBy"
      },
      "outputs": [],
      "source": [
        "# Look at the first few predictions to ensure things went smoothly\n",
        "pd.read_csv('submission.csv').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bADU7avuEMBy"
      },
      "source": [
        "### 1. Visualize and Learn about the Data we are working with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m93VovsDEMBy"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def plot_frequency(tweets_np, labels_np, word_num=10):\n",
        "    # Creates a counter that keeps track of the frequency of words (similar to defaultdict)\n",
        "    pos_counter = Counter()\n",
        "    neg_counter = Counter()\n",
        "\n",
        "    tweets_np = np.copy(tweets_np) # Deep Copy of input\n",
        "\n",
        "    # Go through the tweets dataset\n",
        "    for entry_index in range(tweets_np.shape[0]):\n",
        "        # Flatten all of the features into a single string\n",
        "        words = ' '.join([str(feature) for feature in tweets_np[entry_index]])\n",
        "        # Count the frequency of each word\n",
        "        for word in words.split():\n",
        "            word = word.lower()\n",
        "            # Links would count as independent words without this\n",
        "            if word.startswith('http'):\n",
        "                word = '<LINK>'\n",
        "            # Articles would count as independent words without this\n",
        "            elif word in ['the', 'a', 'an']:\n",
        "                word = '<ARTICLE>'\n",
        "            \n",
        "            if labels_np[entry_index]:\n",
        "                pos_counter[word] += 1\n",
        "            else:\n",
        "                neg_counter[word] += 1\n",
        "    \n",
        "    # Place the data into a data frame in sorted order\n",
        "    top_words = pos_counter.most_common(word_num)\n",
        "    result = {} # TODO Add the logic to put word counts into dictionary here\n",
        "    word_count_df = pd.DataFrame(data=result, columns=['word', 'pos count', 'neg count'])\n",
        "    display(word_count_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md8HLAAdEMBy"
      },
      "outputs": [],
      "source": [
        "plot_frequency(X_data, Y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "madUS-CfEMBy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "c4c6ce542ed86c85d22b49296a52397aea49807c5462a5cc5776ff93f0a8a801"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}